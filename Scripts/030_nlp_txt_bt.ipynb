{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 030_nlp_txt_bt\n",
    "### NLP and Text Bot \n",
    "### James Wilson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Full Name</th>\n",
       "      <th>Final Score</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Hometown</th>\n",
       "      <th>Date</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>dt_indx</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>anecdote</th>\n",
       "      <th>...</th>\n",
       "      <th>lng</th>\n",
       "      <th>population</th>\n",
       "      <th>density</th>\n",
       "      <th>source</th>\n",
       "      <th>military</th>\n",
       "      <th>incorporated</th>\n",
       "      <th>timezone</th>\n",
       "      <th>ranking</th>\n",
       "      <th>zips</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jason Zuffranieri</td>\n",
       "      <td>27600</td>\n",
       "      <td>a math teacher</td>\n",
       "      <td>Albuquerque, New Mexico</td>\n",
       "      <td>2019-07-26</td>\n",
       "      <td>Answer3</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>A movie editor in France has a similar name to...</td>\n",
       "      <td>...</td>\n",
       "      <td>-106.6464</td>\n",
       "      <td>758523.0</td>\n",
       "      <td>1151</td>\n",
       "      <td>polygon</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>America/Denver</td>\n",
       "      <td>2</td>\n",
       "      <td>87121 87120 87123 87112 87113 87110 87111 8711...</td>\n",
       "      <td>1840019176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jason Zuffranieri</td>\n",
       "      <td>4400</td>\n",
       "      <td>a math teacher</td>\n",
       "      <td>Albuquerque, New Mexico</td>\n",
       "      <td>2019-07-25</td>\n",
       "      <td>Answer3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>It took me seconds to 'win' a game of anti-chess.</td>\n",
       "      <td>...</td>\n",
       "      <td>-106.6464</td>\n",
       "      <td>758523.0</td>\n",
       "      <td>1151</td>\n",
       "      <td>polygon</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>America/Denver</td>\n",
       "      <td>2</td>\n",
       "      <td>87121 87120 87123 87112 87113 87110 87111 8711...</td>\n",
       "      <td>1840019176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jason Zuffranieri</td>\n",
       "      <td>30000</td>\n",
       "      <td>a math teacher</td>\n",
       "      <td>Albuquerque, New Mexico</td>\n",
       "      <td>2019-07-24</td>\n",
       "      <td>Answer3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>“In Mexico, I was mistaken for Nicolas Cage.”</td>\n",
       "      <td>...</td>\n",
       "      <td>-106.6464</td>\n",
       "      <td>758523.0</td>\n",
       "      <td>1151</td>\n",
       "      <td>polygon</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>America/Denver</td>\n",
       "      <td>2</td>\n",
       "      <td>87121 87120 87123 87112 87113 87110 87111 8711...</td>\n",
       "      <td>1840019176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jason Zuffranieri</td>\n",
       "      <td>12400</td>\n",
       "      <td>a math teacher</td>\n",
       "      <td>Albuquerque, New Mexico</td>\n",
       "      <td>2019-07-23</td>\n",
       "      <td>Answer3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>My stuffed manatee is a comfort animal in my c...</td>\n",
       "      <td>...</td>\n",
       "      <td>-106.6464</td>\n",
       "      <td>758523.0</td>\n",
       "      <td>1151</td>\n",
       "      <td>polygon</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>America/Denver</td>\n",
       "      <td>2</td>\n",
       "      <td>87121 87120 87123 87112 87113 87110 87111 8711...</td>\n",
       "      <td>1840019176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jason Zuffranieri</td>\n",
       "      <td>18600</td>\n",
       "      <td>a math teacher</td>\n",
       "      <td>Albuquerque, New Mexico</td>\n",
       "      <td>2019-07-22</td>\n",
       "      <td>Answer3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>I travel around the world playing Sudoku.</td>\n",
       "      <td>...</td>\n",
       "      <td>-106.6464</td>\n",
       "      <td>758523.0</td>\n",
       "      <td>1151</td>\n",
       "      <td>polygon</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>America/Denver</td>\n",
       "      <td>2</td>\n",
       "      <td>87121 87120 87123 87112 87113 87110 87111 8711...</td>\n",
       "      <td>1840019176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Full Name  Final Score      Occupation                 Hometown  \\\n",
       "0  Jason Zuffranieri        27600  a math teacher  Albuquerque, New Mexico   \n",
       "1  Jason Zuffranieri         4400  a math teacher  Albuquerque, New Mexico   \n",
       "2  Jason Zuffranieri        30000  a math teacher  Albuquerque, New Mexico   \n",
       "3  Jason Zuffranieri        12400  a math teacher  Albuquerque, New Mexico   \n",
       "4  Jason Zuffranieri        18600  a math teacher  Albuquerque, New Mexico   \n",
       "\n",
       "         Date answer_number  dt_indx  favorite_count  retweet_count  \\\n",
       "0  2019-07-26       Answer3        1              23              2   \n",
       "1  2019-07-25       Answer3        1               6              1   \n",
       "2  2019-07-24       Answer3        1               8              3   \n",
       "3  2019-07-23       Answer3        1               5              0   \n",
       "4  2019-07-22       Answer3        1               6              0   \n",
       "\n",
       "                                            anecdote  ...       lng  \\\n",
       "0  A movie editor in France has a similar name to...  ... -106.6464   \n",
       "1  It took me seconds to 'win' a game of anti-chess.  ... -106.6464   \n",
       "2      “In Mexico, I was mistaken for Nicolas Cage.”  ... -106.6464   \n",
       "3  My stuffed manatee is a comfort animal in my c...  ... -106.6464   \n",
       "4          I travel around the world playing Sudoku.  ... -106.6464   \n",
       "\n",
       "  population density   source  military incorporated        timezone ranking  \\\n",
       "0   758523.0    1151  polygon     False         True  America/Denver       2   \n",
       "1   758523.0    1151  polygon     False         True  America/Denver       2   \n",
       "2   758523.0    1151  polygon     False         True  America/Denver       2   \n",
       "3   758523.0    1151  polygon     False         True  America/Denver       2   \n",
       "4   758523.0    1151  polygon     False         True  America/Denver       2   \n",
       "\n",
       "                                                zips          id  \n",
       "0  87121 87120 87123 87112 87113 87110 87111 8711...  1840019176  \n",
       "1  87121 87120 87123 87112 87113 87110 87111 8711...  1840019176  \n",
       "2  87121 87120 87123 87112 87113 87110 87111 8711...  1840019176  \n",
       "3  87121 87120 87123 87112 87113 87110 87111 8711...  1840019176  \n",
       "4  87121 87120 87123 87112 87113 87110 87111 8711...  1840019176  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "jeopardy_df = pd.read_csv(\"../Data/clean_jeopardy_data.csv\")\n",
    "jeopardy_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text2'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-89a3f2c2d3a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# stop words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mstop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mjeopardy_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text3'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjeopardy_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# frequent words - maybe edit this manually\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2993\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2994\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2995\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2996\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2997\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2897\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2899\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text2'"
     ]
    }
   ],
   "source": [
    "# lower and punctuation \n",
    "#jeopardy_df['text1'] = jeopardy_df['anecdote'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "#jeopardy_df['text1'].head()\n",
    "#jeopardy_df['text1'] = jeopardy_df['text'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# stop words\n",
    "stop = stopwords.words('english')\n",
    "jeopardy_df['text3'] = jeopardy_df['text2'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "# frequent words - maybe edit this manually\n",
    "freq = pd.Series(' '.join(jeopardy_df['text3']).split()).value_counts()[:20]\n",
    "freq = list(freq.index)\n",
    "jeopardy_df['text4'] = jeopardy_df['text3'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "\n",
    "# non frequent\n",
    "nonfreq = pd.Series(' '.join(jeopardy_df['text4']).split()).value_counts()[-10:]\n",
    "nonfreq = list(nonfreq.index)\n",
    "jeopardy_df['text5'] = jeopardy_df['text4'].apply(lambda x: \" \".join(x for x in x.split() if x not in nonfreq))\n",
    "jeopardy_df['text5'].head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check spelling - doesn't work well\n",
    "from textblob import TextBlob\n",
    "#jeopardy_df['text6'] = jeopardy_df['text5'].apply(lambda x: str(TextBlob(x).correct()))\n",
    "# just create word vector\n",
    "#jeopardy_df['text6'] = TextBlob(jeopardy_df['text5']).words\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()\n",
    "jeopardy_df['text5'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "\n",
    "from textblob import Word\n",
    "jeopardy_df['text6'] = jeopardy_df['text5'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "jeopardy_df['text6'].head()\n",
    "\n",
    "TextBlob(jeopardy_df['text6'][0]).ngrams(2)\n",
    "# scikit-learn api\n",
    "\n",
    "# Clean text - spelling / jeopardy handles / additional words / etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = list(jeopardy_df['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jwilson2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jwilson2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jwilson2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jwilson2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jwilson2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jwilson2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jwilson2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jwilson2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jwilson2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jwilson2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jwilson2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "Training on 138,815 character sequences.\n",
      "WARNING:tensorflow:From C:\\Users\\jwilson2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\jwilson2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jwilson2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch 1/15\n",
      "1084/1084 [==============================] - 347s 320ms/step - loss: 1.6187\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "I was a compant to proposed a lot.\n",
      "\n",
      "I was a bardened looked at a communing compants at my mom in the course.\n",
      "\n",
      "I was a comments of my wife and I was a companter at a comments.\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "I couldnt at a land in the sum.\n",
      "\n",
      "I met my wife and a first contestry how it with New York.\n",
      "\n",
      "I was now at my wife and I recommented.\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "My wife is sire cambablawling.\n",
      "\n",
      "I studied I complain afrair\n",
      "\n",
      "I work imulation after Ken Pork.\n",
      "\n",
      "Epoch 2/15\n",
      "1084/1084 [==============================] - 346s 319ms/step - loss: 1.4516\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "I was a bank love and became a bar in my family.\n",
      "\n",
      "I was in a character after the contestons.\n",
      "\n",
      "I was a bad character on Jeopardy\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "I was stranded in a many library\n",
      "\n",
      "I was a bank because I backpacked in franching.\n",
      "\n",
      "I was a pour-trivia to be a college.\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "I stilly browie boys in Francis of Aces.\n",
      "\n",
      "I proposed to be a Brack Almyor gifts.\n",
      "\n",
      "I for a game student and us to grear.\n",
      "\n",
      "Epoch 3/15\n",
      "1084/1084 [==============================] - 356s 328ms/step - loss: 1.3838\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "I was a balle at a band of my husband.\n",
      "\n",
      "I was a college college college.\n",
      "\n",
      "I have a lot of the Grand Schunal and Alex.\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "I took a school of a band in my biologity.\n",
      "\n",
      "I have my husband and college had a balloor.\n",
      "\n",
      "I got my husband at a childrens.\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "My grandmol won helped me  Ken Game.\n",
      "\n",
      "I was suitical cookie at Laeball.\n",
      "\n",
      "I visited Jeopardy eat to alloctast the marathine.\n",
      "\n",
      "Epoch 4/15\n",
      "1084/1084 [==============================] - 352s 324ms/step - loss: 1.3288\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "I want to be a contest of a card man.\n",
      "\n",
      "I want to studied for a competition of my students.\n",
      "\n",
      "I studied and my wife and my students to my wife.\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "I took a student competition\n",
      "\n",
      "I dont know and written students.\n",
      "\n",
      "My dads train goes to a blind.\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "Im a day to Calify Roll Twunny Jer Jimb luncestria.\n",
      "\n",
      "Im always wrote 6 complite toger.\n",
      "\n",
      "Ive quizty evle to study abrist spressie.”\n",
      "\n",
      "Epoch 5/15\n",
      "1084/1084 [==============================] - 345s 318ms/step - loss: 1.2773\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "I have a college company\n",
      "\n",
      "I was a mountaine and was a comment on my students.\n",
      "\n",
      "I was a college company on my students.\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "I took a bar math to see the high school.\n",
      "\n",
      "I played a honeymooned solar\n",
      "\n",
      "I want to do a table college comica.\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "I ilted in Bowlest Batchole Wife.\n",
      "\n",
      "I feel named after a jumper\n",
      "\n",
      "I support diffired point.\n",
      "\n",
      "Epoch 6/15\n",
      "1084/1084 [==============================] - 436s 402ms/step - loss: 1.2331\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "I was a contest of my students to my first contest.\n",
      "\n",
      "I was a book at the best contest\n",
      "\n",
      "I was a book before I was a big country.\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "I liked my birthday with my first birthday.\n",
      "\n",
      "I was a climb dog at the contest of the New York strangers.\n",
      "\n",
      "I was a N Y R New Yearboard\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "I lip with my country for tooto Carbona.\n",
      "\n",
      "I helped code school students in the wrong ring the stuck.\n",
      "\n",
      "I work about my hamal and but wrote\n",
      "\n",
      "Epoch 7/15\n",
      "1084/1084 [==============================] - 402s 371ms/step - loss: 1.1913\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "I was a band at a student to my first dad at a base trial dance.\n",
      "\n",
      "I want to do anything at a bar in a college.\n",
      "\n",
      "I want to see a college proposal to see the hospital team.\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "I like to read any librarian reading.\n",
      "\n",
      "I was a book with my future husband at a bag.\n",
      "\n",
      "My dad was a school band at a band theatre and record.\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "Mad sauges on my reception precipinge.\n",
      "\n",
      "I denoct in a chocolate\n",
      "\n",
      "My mom tenning me a painting over 5 ambaic.\n",
      "\n",
      "Epoch 8/15\n",
      "1084/1084 [==============================] - 388s 358ms/step - loss: 1.1497\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "I was a cat to the college political champion.\n",
      "\n",
      "I thought I was a college program.\n",
      "\n",
      "I want to go to Chicago Chicago in Ireland.\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "I played at the camp to play with my husband.\n",
      "\n",
      "I was a program in the Navy.\n",
      "\n",
      "I stuped a book in the Marathon and I detaile to play.\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My husband have my moms assul\n",
      "\n",
      "I program actinns for a roller Hiddivore.\n",
      "\n",
      "I recited a Dunzaria thought\n",
      "\n",
      "Epoch 9/15\n",
      "1084/1084 [==============================] - 367s 338ms/step - loss: 1.1087\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "I was a contest for the contest for my husband.\n",
      "\n",
      "I was a ballet based on my family.\n",
      "\n",
      "I was a first date of my first date state.\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "I want to go to the Romansida Rivshet.\n",
      "\n",
      "“I have a car and I was a ballet.”\n",
      "\n",
      "I was a cake on a food.\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "“I smart a femeticionors fear of practiceer.\n",
      "\n",
      "Im a smith womens bearded cliff\n",
      "\n",
      "Illogican presidents.\n",
      "\n",
      "Epoch 10/15\n",
      "1084/1084 [==============================] - 22645s 21s/step - loss: 1.0682\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "I was a bar about a college tour.\n",
      "\n",
      "I was a bank for a man.\n",
      "\n",
      "I was a red world tattooed for my family.\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "I like to get my mom with my family research.\n",
      "\n",
      "I was in a band on a card with messy after mistaken for my wife\n",
      "\n",
      "I was in a bar baseball tonte on a brother.\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "I think all Iris Bris Chinas and Droniana.\n",
      "\n",
      "My thesis had a vale government\n",
      "\n",
      "I got humanealy fights in an Impepsive of Morocc-standian fan.\n",
      "\n",
      "Epoch 11/15\n",
      "1084/1084 [==============================] - 333s 307ms/step - loss: 1.0290\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "I was a college room in the Natlog Staff.\n",
      "\n",
      "I was a book for a student to my first date.\n",
      "\n",
      "I was a book for the first time at a ball.\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "I have a charity at age 7.\n",
      "\n",
      "I was a noter with my exchanges for the contest.\n",
      "\n",
      "I was in a room for 1968 conferences.\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "I am an Oaries now.\n",
      "\n",
      "Im on an hospital price.\n",
      "\n",
      "I celebreked in Koundian M as TV\n",
      "\n",
      "Epoch 12/15\n",
      "1084/1084 [==============================] - 393s 362ms/step - loss: 0.9906\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "I was a state ball to speak before here.\n",
      "\n",
      "I saw a company for a station with my family.\n",
      "\n",
      "I was a comment and she wanted to be a crazy company.\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "I was a commicied the Course Flies and science me.”\n",
      "\n",
      "I was a domestic for a safari for my family.\n",
      "\n",
      "I called my school balls and started being a station for a kid.\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "I open a Rolling familyze dumbest.\n",
      "\n",
      "“My school loves demo harm.”\n",
      "\n",
      "I help companies Benny Clinton.\n",
      "\n",
      "Epoch 13/15\n",
      "1084/1084 [==============================] - 347s 320ms/step - loss: 0.9535\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "I was a contest in a roller college.\n",
      "\n",
      "I was a cat at a company.\n",
      "\n",
      "I was a college room at the first time.\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "I have a trivia team court comedy.\n",
      "\n",
      "I took a Sandy and Clinton in a work.\n",
      "\n",
      "I did many comic bass in Canada.\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "I cant turned my friends brawn at Assania.\n",
      "\n",
      "I found my wife at a dance same\n",
      "\n",
      "I was a factor-time if I think the mobiliny concert.\n",
      "\n",
      "Epoch 14/15\n",
      "1084/1084 [==============================] - 349s 322ms/step - loss: 0.9187\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "I have a college rock band company.\n",
      "\n",
      "I was a band at a bar and speed to my bachelors club.\n",
      "\n",
      "I was a company for a company for a lot.\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "I want to ride a quilt of a nuclear college rower.\n",
      "\n",
      "I love to be a company.\n",
      "\n",
      "I love all the show who wants to play but hated me.\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "“I killed Ben speaks. zoo rolled.”\n",
      "\n",
      "I learned face because Irish childres.\n",
      "\n",
      "Ive been to a question fire\n",
      "\n",
      "Epoch 15/15\n",
      "1084/1084 [==============================] - 405s 374ms/step - loss: 0.8872\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "I was a college room of a college tour guide.\n",
      "\n",
      "I was a college room at the contest.\n",
      "\n",
      "I was a college room at the same state contest.\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "I took a boat in the New York in college.\n",
      "\n",
      "I was a childhood times.\n",
      "\n",
      "I love to the best contest.\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "I do elephed my friends.\n",
      "\n",
      "I couldnt feel my game string at sometimes eclipse the police.\n",
      "\n",
      "I rode in running novels\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## USING TEXTGENRNN\n",
    "from textgenrnn import textgenrnn\n",
    "textgen = textgenrnn()\n",
    "textgen.train_on_texts(tweets, num_epochs = 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:08,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was a bank on my name in Barbado in my winnings.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [00:01<00:07,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was in a retreat Nat Gerfy Will Ferry.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [00:04<00:10,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was a college roller descrilition.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [00:06<00:09,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was a big contestant.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [00:14<00:17,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love the world in a trivia contest.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [00:16<00:11,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I study disease from the Navy Rose_14 bitch.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [00:16<00:06,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a cat chicken bands\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [00:17<00:03,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was in a relay rock baseball comedy.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [00:20<00:02,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was a descripter in a cookbook.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:20<00:00,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do South Africa in the world in a state.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "textgen.generate(10, temperature = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Discussion] The house of the street shooting on the state of the state of teammates.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the second program loads a trained net and generates output\n",
    "#from textgenrnn import textgenrnn\n",
    "#textgen = textgenrnn('textgenrnn_weights.hdf5')\n",
    "#textgen.save('textgenrnn_weights.hdf5')\n",
    "#textgen.train_on_texts(tweets, num_epochs = 10)\n",
    "#textgen.generate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot create group in read only mode.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-84ccbdf5554d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'textgenrnn_weights.hdf5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36m_deserialize_model\u001b[1;34m(f, custom_objects, compile)\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m     \u001b[0mmodel_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model_config'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmodel_config\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No model found in config.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\utils\\io_utils.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    300\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_only\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot create group in read only mode.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m                 \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot create group in read only mode."
     ]
    }
   ],
   "source": [
    "#from keras.models import load_model\n",
    "#model = load_model('textgenrnn_weights.hdf5')\n",
    "#for layer in model.layers:\n",
    "#    if len(layer.weights) > 0:\n",
    "#        print(layer.name, layer.weights[0].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
