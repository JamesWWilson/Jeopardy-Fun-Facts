{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 030_nlp_txt_bt\n",
    "### NLP and Text Bot \n",
    "### James Wilson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Full Name</th>\n",
       "      <th>Final Score</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Hometown</th>\n",
       "      <th>Date</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>dt_indx</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>anecdote</th>\n",
       "      <th>...</th>\n",
       "      <th>lng</th>\n",
       "      <th>population</th>\n",
       "      <th>density</th>\n",
       "      <th>source</th>\n",
       "      <th>military</th>\n",
       "      <th>incorporated</th>\n",
       "      <th>timezone</th>\n",
       "      <th>ranking</th>\n",
       "      <th>zips</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jason Zuffranieri</td>\n",
       "      <td>27600</td>\n",
       "      <td>a math teacher</td>\n",
       "      <td>Albuquerque, New Mexico</td>\n",
       "      <td>2019-07-26</td>\n",
       "      <td>Answer3</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>A movie editor in France has a similar name to...</td>\n",
       "      <td>...</td>\n",
       "      <td>-106.6464</td>\n",
       "      <td>758523.0</td>\n",
       "      <td>1151</td>\n",
       "      <td>polygon</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>America/Denver</td>\n",
       "      <td>2</td>\n",
       "      <td>87121 87120 87123 87112 87113 87110 87111 8711...</td>\n",
       "      <td>1840019176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jason Zuffranieri</td>\n",
       "      <td>4400</td>\n",
       "      <td>a math teacher</td>\n",
       "      <td>Albuquerque, New Mexico</td>\n",
       "      <td>2019-07-25</td>\n",
       "      <td>Answer3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>It took me seconds to 'win' a game of anti-chess.</td>\n",
       "      <td>...</td>\n",
       "      <td>-106.6464</td>\n",
       "      <td>758523.0</td>\n",
       "      <td>1151</td>\n",
       "      <td>polygon</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>America/Denver</td>\n",
       "      <td>2</td>\n",
       "      <td>87121 87120 87123 87112 87113 87110 87111 8711...</td>\n",
       "      <td>1840019176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jason Zuffranieri</td>\n",
       "      <td>30000</td>\n",
       "      <td>a math teacher</td>\n",
       "      <td>Albuquerque, New Mexico</td>\n",
       "      <td>2019-07-24</td>\n",
       "      <td>Answer3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>“In Mexico, I was mistaken for Nicolas Cage.”</td>\n",
       "      <td>...</td>\n",
       "      <td>-106.6464</td>\n",
       "      <td>758523.0</td>\n",
       "      <td>1151</td>\n",
       "      <td>polygon</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>America/Denver</td>\n",
       "      <td>2</td>\n",
       "      <td>87121 87120 87123 87112 87113 87110 87111 8711...</td>\n",
       "      <td>1840019176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jason Zuffranieri</td>\n",
       "      <td>12400</td>\n",
       "      <td>a math teacher</td>\n",
       "      <td>Albuquerque, New Mexico</td>\n",
       "      <td>2019-07-23</td>\n",
       "      <td>Answer3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>My stuffed manatee is a comfort animal in my c...</td>\n",
       "      <td>...</td>\n",
       "      <td>-106.6464</td>\n",
       "      <td>758523.0</td>\n",
       "      <td>1151</td>\n",
       "      <td>polygon</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>America/Denver</td>\n",
       "      <td>2</td>\n",
       "      <td>87121 87120 87123 87112 87113 87110 87111 8711...</td>\n",
       "      <td>1840019176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jason Zuffranieri</td>\n",
       "      <td>18600</td>\n",
       "      <td>a math teacher</td>\n",
       "      <td>Albuquerque, New Mexico</td>\n",
       "      <td>2019-07-22</td>\n",
       "      <td>Answer3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>I travel around the world playing Sudoku.</td>\n",
       "      <td>...</td>\n",
       "      <td>-106.6464</td>\n",
       "      <td>758523.0</td>\n",
       "      <td>1151</td>\n",
       "      <td>polygon</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>America/Denver</td>\n",
       "      <td>2</td>\n",
       "      <td>87121 87120 87123 87112 87113 87110 87111 8711...</td>\n",
       "      <td>1840019176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Full Name  Final Score      Occupation                 Hometown  \\\n",
       "0  Jason Zuffranieri        27600  a math teacher  Albuquerque, New Mexico   \n",
       "1  Jason Zuffranieri         4400  a math teacher  Albuquerque, New Mexico   \n",
       "2  Jason Zuffranieri        30000  a math teacher  Albuquerque, New Mexico   \n",
       "3  Jason Zuffranieri        12400  a math teacher  Albuquerque, New Mexico   \n",
       "4  Jason Zuffranieri        18600  a math teacher  Albuquerque, New Mexico   \n",
       "\n",
       "         Date answer_number  dt_indx  favorite_count  retweet_count  \\\n",
       "0  2019-07-26       Answer3        1              23              2   \n",
       "1  2019-07-25       Answer3        1               6              1   \n",
       "2  2019-07-24       Answer3        1               8              3   \n",
       "3  2019-07-23       Answer3        1               5              0   \n",
       "4  2019-07-22       Answer3        1               6              0   \n",
       "\n",
       "                                            anecdote  ...       lng  \\\n",
       "0  A movie editor in France has a similar name to...  ... -106.6464   \n",
       "1  It took me seconds to 'win' a game of anti-chess.  ... -106.6464   \n",
       "2      “In Mexico, I was mistaken for Nicolas Cage.”  ... -106.6464   \n",
       "3  My stuffed manatee is a comfort animal in my c...  ... -106.6464   \n",
       "4          I travel around the world playing Sudoku.  ... -106.6464   \n",
       "\n",
       "  population density   source  military incorporated        timezone ranking  \\\n",
       "0   758523.0    1151  polygon     False         True  America/Denver       2   \n",
       "1   758523.0    1151  polygon     False         True  America/Denver       2   \n",
       "2   758523.0    1151  polygon     False         True  America/Denver       2   \n",
       "3   758523.0    1151  polygon     False         True  America/Denver       2   \n",
       "4   758523.0    1151  polygon     False         True  America/Denver       2   \n",
       "\n",
       "                                                zips          id  \n",
       "0  87121 87120 87123 87112 87113 87110 87111 8711...  1840019176  \n",
       "1  87121 87120 87123 87112 87113 87110 87111 8711...  1840019176  \n",
       "2  87121 87120 87123 87112 87113 87110 87111 8711...  1840019176  \n",
       "3  87121 87120 87123 87112 87113 87110 87111 8711...  1840019176  \n",
       "4  87121 87120 87123 87112 87113 87110 87111 8711...  1840019176  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "jeopardy_df = pd.read_csv(\"../data/processed/clean_jeopardy_data.csv\")\n",
    "jeopardy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower and punctuation \n",
    "#jeopardy_df['text1'] = jeopardy_df['anecdote'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "#jeopardy_df['text1'].head()\n",
    "#jeopardy_df['text1'] = jeopardy_df['text'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# stop words\n",
    "#stop = stopwords.words('english')\n",
    "#jeopardy_df['text3'] = jeopardy_df['text2'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "# frequent words - maybe edit this manually\n",
    "#freq = pd.Series(' '.join(jeopardy_df['text3']).split()).value_counts()[:20]\n",
    "#freq = list(freq.index)\n",
    "#jeopardy_df['text4'] = jeopardy_df['text3'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "\n",
    "# non frequent\n",
    "#nonfreq = pd.Series(' '.join(jeopardy_df['text4']).split()).value_counts()[-10:]\n",
    "#nonfreq = list(nonfreq.index)\n",
    "#jeopardy_df['text5'] = jeopardy_df['text4'].apply(lambda x: \" \".join(x for x in x.split() if x not in nonfreq))\n",
    "#jeopardy_df['text5'].head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check spelling - doesn't work well\n",
    "#from textblob import TextBlob\n",
    "#jeopardy_df['text6'] = jeopardy_df['text5'].apply(lambda x: str(TextBlob(x).correct()))\n",
    "# just create word vector\n",
    "#jeopardy_df['text6'] = TextBlob(jeopardy_df['text5']).words\n",
    "\n",
    "#from nltk.stem import PorterStemmer\n",
    "#st = PorterStemmer()\n",
    "#jeopardy_df['text5'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "\n",
    "#from textblob import Word\n",
    "#jeopardy_df['text6'] = jeopardy_df['text5'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "#jeopardy_df['text6'].head()\n",
    "\n",
    "#TextBlob(jeopardy_df['text6'][0]).ngrams(2)\n",
    "# scikit-learn api\n",
    "\n",
    "# Clean text - spelling / jeopardy handles / additional words / etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = list(jeopardy_df['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jwilson2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jwilson2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jwilson2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jwilson2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jwilson2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jwilson2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jwilson2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jwilson2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jwilson2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jwilson2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jwilson2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "Training on 116,279 character sequences.\n",
      "WARNING:tensorflow:From C:\\Users\\jwilson2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\jwilson2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jwilson2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch 1/10\n",
      "908/908 [==============================] - 623s 686ms/step - loss: 1.6298\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "I was a proposal man at my wife\n",
      "\n",
      "I was a brother and I was a band in my daughter.\n",
      "\n",
      "I was a bad students and I was a broke compation.\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "I loved to because the person women\n",
      "\n",
      "I was proposal map carried.\n",
      "\n",
      "I law the family countrian\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "I played about my first attended.\n",
      "\n",
      "I shelp about Stone Clinta Lamra futura laid 3.\n",
      "\n",
      "I have a Nothings Santertan b➡r\n",
      "\n",
      "Epoch 2/10\n",
      "908/908 [==============================] - 2813s 3s/step - loss: 1.4480\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "I met my family and my grandma in a coloradon.\n",
      "\n",
      "I was a concert to my family and I was a company.\n",
      "\n",
      "I was a college college college country college.\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "I deteing a school guy on a fire place.\n",
      "\n",
      "I had a tournament school at a cook grades and I knit.\n",
      "\n",
      "I have a creat at the Name concert\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "My son cood place meme.\n",
      "\n",
      "I emility at a potter isleancé disophorate\n",
      "\n",
      "I knittee for lunchy to Chorsespa\n",
      "\n",
      "Epoch 3/10\n",
      "908/908 [==============================] - 282s 310ms/step - loss: 1.3653\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "I was a bar care at a bar because I was a contest.\n",
      "\n",
      "I was a contest at a contest in my best teacher.\n",
      "\n",
      "I was a book and watch my wife.\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "I research my cats saved my home in my family.\n",
      "\n",
      "I was an all book on I reads.\n",
      "\n",
      "I was a long bear thing and I was a performance.\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "I made a 32 mile Gh When Phanna.\n",
      "\n",
      "I forced a Robbinf at him in 40.\n",
      "\n",
      "I want to elevenainer\n",
      "\n",
      "Epoch 4/10\n",
      "908/908 [==============================] - 295s 325ms/step - loss: 1.2968\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "I love the Star Wars and Marathon.\n",
      "\n",
      "I was a book on a podcast in a contest.\n",
      "\n",
      "I was a convention of a bar car car.\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "I saw a lacypot on a team.\n",
      "\n",
      "I saw soccer at a sue in a bar ocean.\n",
      "\n",
      "I do making as a contestant law post.\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "I read Chinese on a syris\n",
      "\n",
      "I crauphesdame and Cake States\n",
      "\n",
      "I was a fake bund cracked for my for-my wife.\n",
      "\n",
      "Epoch 5/10\n",
      "908/908 [==============================] - 312s 343ms/step - loss: 1.2353\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "I was a book at a college baraster.\n",
      "\n",
      "I worked at a student and the most power-convention of my students.\n",
      "\n",
      "I was a book of the world book and I was a bar company.\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "I am a book to the born team.\n",
      "\n",
      "I think I was on a book and beer the has a Fortish to be on a card college.\n",
      "\n",
      "I took a start theory of my husband.\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "I wrote a character pie states museum iss.\n",
      "\n",
      "I partistant doing cats\n",
      "\n",
      "My schools at 50 perfoungarys.\n",
      "\n",
      "Epoch 6/10\n",
      "908/908 [==============================] - 382s 421ms/step - loss: 1.1772\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "I was a competition to my students want to be a childhood on a poor.\n",
      "\n",
      "I was a competition of my family on a police.\n",
      "\n",
      "I was a political school champion in my wife.\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "I saw my champion of China.\n",
      "\n",
      "I won a perfect perfume every year.\n",
      "\n",
      "My a children were be a friend and I wasnt to champ.”\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "Im not Breenball With Smangoga of the Critition Jeopardy Made Wearchill.\n",
      "\n",
      "I was in the Arcident, Christmas Potter and Crobr.\n",
      "\n",
      "I rode a bar in a program get.\n",
      "\n",
      "Epoch 7/10\n",
      "908/908 [==============================] - 344s 378ms/step - loss: 1.1224\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "I was a college book and students.\n",
      "\n",
      "I was a college barasty competitive.\n",
      "\n",
      "I was a company for a cat\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "My childrens respect of my research starts.\n",
      "\n",
      "I taught in a competitive teacher.\n",
      "\n",
      "I play all of my wife in a school.\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "Im thank me into a grang\n",
      "\n",
      "I taught Vosters figure surprise\n",
      "\n",
      "I wrote a trip into my wife parts convetics.\n",
      "\n",
      "Epoch 8/10\n",
      "908/908 [==============================] - 355s 391ms/step - loss: 1.0699\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "I was a professional competition to see the song to make it.\n",
      "\n",
      "I was a state of my family and weird states in a hand.\n",
      "\n",
      "I was a book and I was a concert.\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "I was a finalist and love them.\n",
      "\n",
      "I saw a contest in a company.\n",
      "\n",
      "I took a pat and study for a lot of student and I have a patron company.\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I collect twinate for class.\n",
      "\n",
      "Ive played sent-likely to make fushed off it.”\n",
      "\n",
      "Took my strange winning smart speech.\n",
      "\n",
      "Epoch 9/10\n",
      "908/908 [==============================] - 365s 402ms/step - loss: 1.0198\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "I was a contest in a state for my wedding.\n",
      "\n",
      "I was a company at a theme to my students to watch it.\n",
      "\n",
      "I was a college bartender.\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "Ive been to 14 years and admired the over of a catans.\n",
      "\n",
      "I had a pope show who helped me a stamp.\n",
      "\n",
      "I had a proposal contest and trees.\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "My cat finals for a compute mate geography.\n",
      "\n",
      "I went to Univectical Bet.\n",
      "\n",
      "I do.\n",
      "\n",
      "Epoch 10/10\n",
      "908/908 [==============================] - 422s 465ms/step - loss: 0.9752\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "I was a college students to make my wedding in the world.\n",
      "\n",
      "I was a book at a theme band to me.\n",
      "\n",
      "I was a big Spanish and all the world.\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "I was a podcast award of Monster.\n",
      "\n",
      "My mom and I have a children.\n",
      "\n",
      "I love blood lines.\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "Ive been to 14 years at question about cream.\n",
      "\n",
      "I think 3-monther was only tanning.\n",
      "\n",
      "Our sneakers was great program.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## USING TEXTGENRNN\n",
    "from textgenrnn import textgenrnn\n",
    "textgen = textgenrnn()\n",
    "textgen.train_on_texts(tweets, num_epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:04<00:36,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was a first date in a children.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [00:05<00:25,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was a college bartender\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [00:11<00:27,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I play and study first dog with me.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [00:11<00:18,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I got my dad at a Golden Game in Barack.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [00:12<00:11,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My family has the tour crest in my schools.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [00:13<00:07,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love blood with my name in a sun.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [00:14<00:04,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I got a correct cat at the cat\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [00:14<00:02,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was a boat for a powerlifter.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [00:15<00:01,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I collect people who didnt know the cathous.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:16<00:00,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I couldnt student with my students watch win.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "textgen.generate(10, temperature = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# I was a college bartender\n",
    "\n",
    "\n",
    "# I got my dad at a Golden Game in Barack.\n",
    "\n",
    "\n",
    "# I love blood with my name in a sun.\n",
    "\n",
    "\n",
    "# I collect people who didnt know the cathous.\n",
    "# I was a boat for a powerlifter.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Discussion] The house of the street shooting on the state of the state of teammates.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the second program loads a trained net and generates output\n",
    "#from textgenrnn import textgenrnn\n",
    "#textgen = textgenrnn('textgenrnn_weights.hdf5')\n",
    "#textgen.save('textgenrnn_weights.hdf5')\n",
    "#textgen.train_on_texts(tweets, num_epochs = 10)\n",
    "#textgen.generate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot create group in read only mode.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-84ccbdf5554d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'textgenrnn_weights.hdf5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36m_deserialize_model\u001b[1;34m(f, custom_objects, compile)\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m     \u001b[0mmodel_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model_config'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmodel_config\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No model found in config.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\utils\\io_utils.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    300\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_only\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot create group in read only mode.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m                 \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot create group in read only mode."
     ]
    }
   ],
   "source": [
    "#from keras.models import load_model\n",
    "#model = load_model('textgenrnn_weights.hdf5')\n",
    "#for layer in model.layers:\n",
    "#    if len(layer.weights) > 0:\n",
    "#        print(layer.name, layer.weights[0].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
